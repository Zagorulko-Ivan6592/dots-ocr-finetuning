{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dots.ocr: Обработка PDF с дообученной моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Зависимости установлены\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q transformers accelerate qwen-vl-utils\n",
    "!pip install -q pillow tqdm pymupdf pandas markdown pydantic openai requests\n",
    "!pip install -q typing-extensions>=4.12.2\n",
    "print(\"Зависимости установлены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "Память: 85.1 GB\n",
      "Занято: 0.00 GB\n",
      "Свободно: 85.10 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Память: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"Занято: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    print(f\"Свободно: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: GPU не обнаружен, используется CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: OK\n",
      "Модель: OK\n",
      "dots.ocr: OK\n",
      "\n",
      "Режим: vLLM\n",
      "Модель: Дообученная (объединенная)\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"/home/jovyan/kurkin/geo_bench_zagorulko/OCR/DotsOCR_finetuned\"\n",
    "PDF_PATH = \"/home/jovyan/kurkin/geo_bench_zagorulko/OCR/data/TAR_ie_2023_12_06.pdf\"\n",
    "OUT_DIR = \"/home/jovyan/kurkin/geo_bench_zagorulko/OCR/outputs\"\n",
    "DOTS_OCR_PATH = \"/home/jovyan/kurkin/geo_bench_zagorulko/OCR/dots_ocr\"\n",
    "\n",
    "# Параметры\n",
    "DPI = 200\n",
    "PROMPT_MODE = \"prompt_layout_all_en\"  \n",
    "MAX_NEW_TOKENS = 24000\n",
    "USE_FAST_INFERENCE = True\n",
    "\n",
    "USE_VLLM = True\n",
    "VLLM_SERVER_IP = \"localhost\"\n",
    "VLLM_SERVER_PORT = 8000\n",
    "VLLM_PROTOCOL = \"http\"\n",
    "VLLM_MODEL_NAME = MODEL_PATH\n",
    "\n",
    "from pathlib import Path # Проверка путей\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "errors = []\n",
    "for name, path in [(\"PDF\", PDF_PATH), (\"Модель\", MODEL_PATH), (\"dots.ocr\", DOTS_OCR_PATH)]:\n",
    "    exists = Path(path).exists()\n",
    "    print(f\"{name}: {'OK' if exists else 'ERROR'}\")\n",
    "    if not exists:\n",
    "        errors.append(f\"{name}: {path}\")\n",
    "\n",
    "if errors:\n",
    "    raise FileNotFoundError(f\"Не найдены пути: {errors}\")\n",
    "\n",
    "print(f\"\\nРежим: {'vLLM' if USE_VLLM else 'HF Transformers'}\")\n",
    "print(f\"Модель: Дообученная (объединенная)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подключение dots.ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dots.ocr подключен\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path(DOTS_OCR_PATH)))\n",
    "\n",
    "try:\n",
    "    import dots_ocr\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"-e\", str(DOTS_OCR_PATH)], check=False)\n",
    "\n",
    "try:\n",
    "    from dots_ocr import DotsOCRParser\n",
    "except ImportError:\n",
    "    from dots_ocr.parser import DotsOCRParser\n",
    "\n",
    "print(\"dots.ocr подключен\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM сервер доступен\n",
      "use vllm model, num_thread will be set to 4\n",
      "Парсер с vLLM готов\n"
     ]
    }
   ],
   "source": [
    "if USE_VLLM:\n",
    "    try:\n",
    "        import requests\n",
    "        test_url = f\"{VLLM_PROTOCOL}://{VLLM_SERVER_IP}:{VLLM_SERVER_PORT}/health\"\n",
    "        response = requests.get(test_url, timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            print(\"vLLM сервер доступен\")\n",
    "        else:\n",
    "            print(f\"WARNING: vLLM сервер статус {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: vLLM сервер недоступен: {e}\")\n",
    "    \n",
    "    parser = DotsOCRParser(\n",
    "        protocol=VLLM_PROTOCOL, ip=VLLM_SERVER_IP, port=VLLM_SERVER_PORT,\n",
    "        model_name=VLLM_MODEL_NAME, use_hf=False,\n",
    "        dpi=DPI, output_dir=OUT_DIR, num_thread=4\n",
    "    )\n",
    "    print(\"Парсер с vLLM готов\")\n",
    "else:\n",
    "    parser = DotsOCRParser(use_hf=True, dpi=DPI, output_dir=OUT_DIR)\n",
    "    # Переопределяем путь к модели для HF режима\n",
    "    parser.model_path = MODEL_PATH\n",
    "    print(\"Парсер с HF Transformers готов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pdf: /home/jovyan/kurkin/geo_bench_zagorulko/OCR/data/TAR_ie_2023_12_06.pdf\n",
      "Parsing PDF with 42 pages using 4 threads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages: 100%|██████████| 42/42 [01:48<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing finished, results saving to /home/jovyan/kurkin/geo_bench_zagorulko/OCR/outputs/TAR_ie_2023_12_06\n",
      "\n",
      "Готово! Результаты в /home/jovyan/kurkin/geo_bench_zagorulko/OCR/outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = parser.parse_file(PDF_PATH, prompt_mode=PROMPT_MODE, fitz_preprocess=False)\n",
    "print(f\"\\nГотово! Результаты в {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Объединение Markdown и восстановление PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используем JSON файлы для восстановления (более точный порядок элементов)\n",
      "Объединено 42 страниц\n",
      "Markdown сохранен: /home/jovyan/kurkin/geo_bench_zagorulko/OCR/outputs/TAR_ie_2023_12_06_combined.md\n",
      "HTML сохранен: /home/jovyan/kurkin/geo_bench_zagorulko/OCR/outputs/TAR_ie_2023_12_06_combined.html\n",
      "PDF восстановлен: /home/jovyan/kurkin/geo_bench_zagorulko/OCR/outputs/TAR_ie_2023_12_06_reconstructed.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import markdown as md\n",
    "from PIL import Image\n",
    "\n",
    "pdf_filename = Path(PDF_PATH).stem\n",
    "save_dir = Path(OUT_DIR) / pdf_filename\n",
    "\n",
    "json_files = list(save_dir.glob(\"*_page_*.json\")) if save_dir.exists() else []\n",
    "md_files = list(save_dir.glob(\"*_page_*.md\")) if save_dir.exists() else []\n",
    "\n",
    "def extract_page_num(filename):\n",
    "    match = re.search(r'_page_(\\d+)', filename.stem)\n",
    "    return int(match.group(1)) if match else 999\n",
    "\n",
    "json_files.sort(key=extract_page_num)\n",
    "valid_md_files = [f for f in md_files if not f.name.endswith(\"_nohf.md\")]\n",
    "valid_md_files.sort(key=extract_page_num)\n",
    "\n",
    "if json_files or valid_md_files:\n",
    "    combined_md = []\n",
    "    combined_md.append(f\"# {pdf_filename}\\n\\n\")\n",
    "    combined_md.append(f\"*Объединенный документ из {max(len(json_files), len(valid_md_files))} страниц*\\n\\n\")\n",
    "    combined_md.append(\"---\\n\\n\")\n",
    "    \n",
    "    if json_files:\n",
    "        print(\"Используем JSON файлы для восстановления (более точный порядок элементов)\")\n",
    "        try:\n",
    "            from dots_ocr.utils.format_transformer import layoutjson2md\n",
    "            \n",
    "            for json_file in json_files:\n",
    "                page_num = extract_page_num(json_file)\n",
    "                \n",
    "                # Загружаем JSON\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    cells = json.load(f)\n",
    "                \n",
    "                # Загружаем соответствующее изображение страницы\n",
    "                img_path = json_file.with_suffix('.jpg')\n",
    "                if not img_path.exists():\n",
    "                    img_path = json_file.parent / f\"{json_file.stem.replace('.json', '')}.jpg\"\n",
    "                \n",
    "                if img_path.exists():\n",
    "                    origin_image = Image.open(img_path)\n",
    "                    # Генерируем Markdown из JSON с правильным порядком\n",
    "                    md_content = layoutjson2md(origin_image, cells, text_key='text', no_page_hf=True)\n",
    "                else:\n",
    "                    # Если нет изображения, используем готовый MD файл\n",
    "                    md_file = json_file.with_suffix('.md')\n",
    "                    if md_file.exists():\n",
    "                        md_content = md_file.read_text(encoding='utf-8')\n",
    "                    else:\n",
    "                        # Просто текст из JSON\n",
    "                        md_content = \"\\n\\n\".join([cell.get('text', '') for cell in cells if cell.get('text')])\n",
    "                \n",
    "                combined_md.append(f\"## Страница {page_num}\\n\\n\")\n",
    "                combined_md.append(md_content)\n",
    "                combined_md.append(\"\\n\\n---\\n\\n\")\n",
    "        except ImportError:\n",
    "            print(\"Не удалось импортировать layoutjson2md, используем MD файлы\")\n",
    "            json_files = []\n",
    "    \n",
    "    if not json_files and valid_md_files:\n",
    "        print(\"Используем MD файлы для восстановления\")\n",
    "        for md_file in valid_md_files:\n",
    "            page_num = extract_page_num(md_file)\n",
    "            content = md_file.read_text(encoding=\"utf-8\")\n",
    "            combined_md.append(f\"## Страница {page_num}\\n\\n\")\n",
    "            combined_md.append(content)\n",
    "            combined_md.append(\"\\n\\n---\\n\\n\")\n",
    "    \n",
    "    combined_md_text = \"\".join(combined_md)\n",
    "    combined_md_path = Path(OUT_DIR) / f\"{pdf_filename}_combined.md\"\n",
    "    combined_md_path.write_text(combined_md_text, encoding=\"utf-8\")\n",
    "    print(f\"Объединено {max(len(json_files), len(valid_md_files))} страниц\")\n",
    "    print(f\"Markdown сохранен: {combined_md_path}\")\n",
    "    \n",
    "    html_content = md.markdown(combined_md_text, extensions=[\"tables\", \"fenced_code\", \"codehilite\"])\n",
    "    html_with_style = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "        th {{ background-color: #f2f2f2; }}\n",
    "        code {{ background-color: #f4f4f4; padding: 2px 4px; border-radius: 3px; }}\n",
    "        pre {{ background-color: #f4f4f4; padding: 10px; border-radius: 5px; overflow-x: auto; }}\n",
    "        img {{ max-width: 100%; height: auto; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "{html_content}\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "    \n",
    "    html_path = Path(OUT_DIR) / f\"{pdf_filename}_combined.html\"\n",
    "    html_path.write_text(html_with_style, encoding=\"utf-8\")\n",
    "    print(f\"HTML сохранен: {html_path}\")\n",
    "    \n",
    "    try:\n",
    "        import weasyprint\n",
    "        pdf_path = Path(OUT_DIR) / f\"{pdf_filename}_reconstructed.pdf\"\n",
    "        weasyprint.HTML(string=html_with_style).write_pdf(pdf_path)\n",
    "        print(f\"PDF восстановлен: {pdf_path}\")\n",
    "    except ImportError:\n",
    "        print(\"weasyprint не установлен, пропускаем создание PDF\")\n",
    "        print(\"Установите: pip install weasyprint\")\n",
    "        print(\"Или откройте HTML файл в браузере для проверки\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при создании PDF: {e}\")\n",
    "        print(\"Откройте HTML файл в браузере для проверки\")\n",
    "else:\n",
    "    print(\"JSON и Markdown файлы не найдены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
